task_name: ffhq_inpainting
batch_size: 64
resume: false

logging:
  debug: false
  project: bridge_matching
  #resume: False
  mode: online
  name: ${now:%Y.%m.%d-%H.%M.%S}_${task_name}
  tags: ["${task_name}"]
  id: null
  group: null

model: 
  _target_: bridge_matching.model.BridgeMatchingModel
  predict_type: x_orig

# bridge:
#   _target_: bridge_matching.bridge.BrownianBridge
#   gamma: 0.1

# bridge:
#   _target_: bridge_matching.bridge.GeneralizedBrownianBridge
#   schedule:
#     _target_: bridge_matching.bridge.schedule.TriangularSchedule
#     beta_max: 0.2

bridge:
  _target_: bridge_matching.bridge.FlowMatching

# sampling_params:
#   num_steps: 20
#   vis_steps: 10
#   timesteps: uniform

sampling_params:
  num_steps: 50
  vis_steps: 10
  timesteps: diffusion
  rho: 10
  sigma_max: 1
  sigma_min: 1e-2

loss:
  _target_: bridge_matching.loss.BridgeMatchingLoss
  timestep_sampler: 
    _target_: bridge_matching.loss.LogitSampler
    mean: -2.5

transform:
  _target_: bridge_matching.transform.InpaintingTransform
  noise_std: 0

data:
  train:
    batch_size: ${batch_size}
    num_workers: 5
    datasets:
      - _target_: bridge_matching.dataset.FFHQDataset
        root_dir: datasets/ffhq/train
  test:
    batch_size: ${batch_size}
    num_workers: 5
    datasets:
      - _target_: bridge_matching.dataset.FFHQDataset
        root_dir: datasets/ffhq/test

# data:
#   train:
#     batch_size: ${batch_size}
#     num_workers: 5
#     datasets:
#       - _target_: bridge_matching.dataset.ColoredMNIST
#         root: datasets/colored_mnist
#         train: true
#         download: true
#   test:
#     batch_size: ${batch_size}
#     num_workers: 5
#     datasets:
#       - _target_: bridge_matching.dataset.ColoredMNIST
#         root: datasets/colored_mnist
#         train: false
#         download: true

metrics:
  - _target_: bridge_matching.metric.FIDMetric
    name: FID
    # ref_path: stats/colored_mnist
    ref_path: stats/ffhq
    num_expected: 3200
    batch_size: ${batch_size}

optimizer:
  _target_: torch.optim.Adam
  lr: 0

# lr_scheduler:
#   _target_: torch.optim.lr_scheduler.ExponentialLR
#   gamma: 1

lr_scheduler:
  _target_: bridge_matching.utils.scheduler.WarmUpLRWithDecay
  warmup_steps: 10000
  max_lr: 0.0001
  total_steps: 200000
  decay_rate: 0.99995
  

trainer:
  log_step: 200
  viz_step: 500
  save_epoch: 1
  len_epoch: 4000
  epochs: 50
  grad_norm_clip: 100
  predictions_dir: predicted_images
  output_dir: ./
